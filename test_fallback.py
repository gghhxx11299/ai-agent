#!/usr/bin/env python3
"""
Test script for AI fallback system
Tests the automatic fallback mechanism between Gemini, Groq, and OpenRouter
"""

import asyncio
import os
import sys
from unittest.mock import patch, MagicMock
from rich.console import Console
from rich.table import Table
from config.config import Config
from src.orchestrator import Orchestrator

console = Console()


def print_banner():
    """Print test banner"""
    console.print("\n" + "="*60, style="bold cyan")
    console.print("  ðŸ§ª AI Fallback System Test Suite", style="bold cyan")
    console.print("="*60 + "\n", style="bold cyan")


def print_config_status():
    """Print current API key configuration"""
    table = Table(title="ðŸ“‹ Current API Configuration", show_header=True)
    table.add_column("AI Model", style="cyan")
    table.add_column("Status", style="green")
    table.add_column("API Key", style="dim")

    # Check Gemini
    gemini_status = "âœ“ Configured" if Config.GEMINI_API_KEY else "âœ— Not Set"
    gemini_style = "green" if Config.GEMINI_API_KEY else "red"
    gemini_key = f"{Config.GEMINI_API_KEY[:20]}..." if Config.GEMINI_API_KEY else "Not configured"
    table.add_row("Gemini (Primary)", f"[{gemini_style}]{gemini_status}[/{gemini_style}]", gemini_key)

    # Check Groq
    groq_status = "âœ“ Configured" if Config.GROQ_API_KEY else "âœ— Not Set"
    groq_style = "green" if Config.GROQ_API_KEY else "red"
    groq_key = f"{Config.GROQ_API_KEY[:20]}..." if Config.GROQ_API_KEY else "Not configured"
    table.add_row("Groq (Fallback #1)", f"[{groq_style}]{groq_status}[/{groq_style}]", groq_key)

    # Check OpenRouter
    or_status = "âœ“ Configured" if Config.OPENROUTER_API_KEY else "âœ— Not Set"
    or_style = "green" if Config.OPENROUTER_API_KEY else "red"
    or_key = f"{Config.OPENROUTER_API_KEY[:20]}..." if Config.OPENROUTER_API_KEY else "Not configured"
    table.add_row("OpenRouter (Fallback #2)", f"[{or_style}]{or_status}[/{or_style}]", or_key)

    console.print(table)
    console.print()


async def test_orchestrator_initialization():
    """Test 1: Verify orchestrator initializes with available models"""
    console.print("\n[bold yellow]Test 1: Orchestrator Initialization[/bold yellow]")
    console.print("Testing if orchestrator properly detects and initializes AI models...\n")

    try:
        orchestrator = Orchestrator()

        # Check which models were initialized
        console.print(f"[green]âœ“ Orchestrator initialized successfully[/green]")
        console.print(f"  â€¢ Available models: {len(orchestrator.ai_models)}")
        console.print(f"  â€¢ Current AI: {orchestrator.ai_name}")

        for idx, (name, _) in enumerate(orchestrator.ai_models, 1):
            console.print(f"  â€¢ Model {idx}: {name}")

        return True, orchestrator
    except Exception as e:
        console.print(f"[red]âœ— Initialization failed: {e}[/red]")
        return False, None


async def test_simple_query(orchestrator):
    """Test 2: Simple query with current configuration"""
    console.print("\n[bold yellow]Test 2: Simple Query Processing[/bold yellow]")
    console.print("Testing a simple query that doesn't require external data...\n")

    test_query = "What is NDVI?"

    try:
        console.print(f"Query: [cyan]{test_query}[/cyan]\n")
        response = await orchestrator.process_query(test_query)

        console.print(f"\n[green]âœ“ Query processed successfully[/green]")
        console.print(f"  â€¢ AI used: {orchestrator.ai_name}")
        console.print(f"  â€¢ Response length: {len(response)} characters")
        console.print(f"\n[dim]Response preview:[/dim]")
        console.print(f"[dim]{response[:200]}...[/dim]\n")

        return True
    except Exception as e:
        console.print(f"[red]âœ— Query failed: {e}[/red]")
        return False


async def test_fallback_simulation():
    """Test 3: Simulate model failure and test fallback"""
    console.print("\n[bold yellow]Test 3: Fallback Simulation[/bold yellow]")
    console.print("Simulating primary model failure to test automatic fallback...\n")

    # Check if we have multiple models configured
    model_count = sum([
        1 if Config.GEMINI_API_KEY else 0,
        1 if Config.GROQ_API_KEY else 0,
        1 if Config.OPENROUTER_API_KEY else 0
    ])

    if model_count < 2:
        console.print("[yellow]âš ï¸  Fallback test requires at least 2 AI models configured[/yellow]")
        console.print("[yellow]   Current configuration only has 1 model[/yellow]")
        console.print("\n[cyan]To properly test fallback, add a Groq API key:[/cyan]")
        console.print("  1. Get free API key from: https://console.groq.com/keys")
        console.print("  2. Add to .env: GROQ_API_KEY=your_key_here")
        console.print("  3. Re-run this test\n")
        return False

    try:
        orchestrator = Orchestrator()
        original_model = orchestrator.ai_name

        # Simulate failure of first model
        console.print(f"Simulating failure of {original_model}...\n")

        # Mock the first AI model to fail
        if orchestrator.ai_models:
            original_method = orchestrator.ai_models[0][1].analyze_query

            async def failing_method(*args, **kwargs):
                raise Exception("Simulated API failure")

            orchestrator.ai_models[0] = (
                orchestrator.ai_models[0][0],
                type('MockAI', (), {
                    'analyze_query': failing_method,
                    'synthesize_response': failing_method,
                    'answer_directly': failing_method
                })()
            )

            # Try a query
            test_query = "What is crop rotation?"
            console.print(f"Query: [cyan]{test_query}[/cyan]\n")

            response = await orchestrator.process_query(test_query)

            console.print(f"\n[green]âœ“ Fallback successful![/green]")
            console.print(f"  â€¢ Original model: {original_model}")
            console.print(f"  â€¢ Fallback model: {orchestrator.ai_name}")
            console.print(f"  â€¢ Response received: {len(response)} characters\n")

            return True

    except Exception as e:
        console.print(f"[red]âœ— Fallback test failed: {e}[/red]")
        return False


async def test_all_models_fail():
    """Test 4: Verify error handling when all models fail"""
    console.print("\n[bold yellow]Test 4: All Models Fail Scenario[/bold yellow]")
    console.print("Testing error handling when all AI models are unavailable...\n")

    try:
        # Create orchestrator with mocked failing models
        with patch('src.orchestrator.GeminiIntegration') as mock_gemini, \
             patch('src.orchestrator.GroqIntegration') as mock_groq, \
             patch('src.orchestrator.OpenRouterIntegration') as mock_openrouter:

            # Make all models fail
            async def fail(*args, **kwargs):
                raise Exception("Model unavailable")

            mock_ai = MagicMock()
            mock_ai.analyze_query = fail
            mock_ai.synthesize_response = fail
            mock_ai.answer_directly = fail

            mock_gemini.return_value = mock_ai
            mock_groq.return_value = mock_ai
            mock_openrouter.return_value = mock_ai

            # This should raise an error
            console.print("Attempting to initialize orchestrator with all models failing...")

            # This test is more complex, skipping for now
            console.print("[yellow]âš ï¸  Comprehensive failure test requires mock setup[/yellow]")
            console.print("[dim]The orchestrator should raise: 'All AI models failed'[/dim]\n")

            return None

    except Exception as e:
        console.print(f"[red]Error in test: {e}[/red]")
        return False


async def test_model_switching_persistence():
    """Test 5: Verify that successful fallback model becomes primary"""
    console.print("\n[bold yellow]Test 5: Model Switching Persistence[/bold yellow]")
    console.print("Testing if fallback model is promoted to primary after success...\n")

    model_count = sum([
        1 if Config.GEMINI_API_KEY else 0,
        1 if Config.GROQ_API_KEY else 0,
        1 if Config.OPENROUTER_API_KEY else 0
    ])

    if model_count < 2:
        console.print("[yellow]âš ï¸  This test requires at least 2 AI models configured[/yellow]\n")
        return False

    console.print("[dim]This is tested implicitly in Test 3[/dim]")
    console.print("[dim]When fallback succeeds, the working model is moved to position 0[/dim]\n")

    return None


def print_summary(results):
    """Print test summary"""
    console.print("\n" + "="*60, style="bold cyan")
    console.print("  ðŸ“Š Test Summary", style="bold cyan")
    console.print("="*60 + "\n", style="bold cyan")

    table = Table(show_header=True)
    table.add_column("Test", style="cyan")
    table.add_column("Result", style="green")

    for test_name, result in results.items():
        if result is True:
            status = "[green]âœ“ PASSED[/green]"
        elif result is False:
            status = "[red]âœ— FAILED[/red]"
        else:
            status = "[yellow]âŠ˜ SKIPPED[/yellow]"

        table.add_row(test_name, status)

    console.print(table)
    console.print()


async def main():
    """Run all tests"""
    print_banner()
    print_config_status()

    # Check if we have at least one AI configured
    if not (Config.GEMINI_API_KEY or Config.GROQ_API_KEY or Config.OPENROUTER_API_KEY):
        console.print("[red]âŒ No AI models configured![/red]")
        console.print("[yellow]Please add at least one API key to .env file[/yellow]\n")
        sys.exit(1)

    results = {}

    # Test 1: Initialization
    success, orchestrator = await test_orchestrator_initialization()
    results["1. Orchestrator Initialization"] = success

    if not success:
        console.print("\n[red]Cannot continue tests without successful initialization[/red]\n")
        print_summary(results)
        return

    # Test 2: Simple Query
    results["2. Simple Query Processing"] = await test_simple_query(orchestrator)

    # Test 3: Fallback Simulation
    results["3. Fallback Mechanism"] = await test_fallback_simulation()

    # Test 4: All Models Fail
    results["4. All Models Fail"] = await test_all_models_fail()

    # Test 5: Model Switching Persistence
    results["5. Model Switching Persistence"] = await test_model_switching_persistence()

    # Print summary
    print_summary(results)

    # Recommendations
    console.print("[bold cyan]ðŸ’¡ Recommendations:[/bold cyan]\n")

    if not Config.GROQ_API_KEY:
        console.print("[yellow]1. Add Groq API for fallback:[/yellow]")
        console.print("   â€¢ Get free key: https://console.groq.com/keys")
        console.print("   â€¢ Add to .env: GROQ_API_KEY=your_key")
        console.print()

    if not Config.OPENROUTER_API_KEY:
        console.print("[yellow]2. Add OpenRouter API for additional fallback:[/yellow]")
        console.print("   â€¢ Get key: https://openrouter.ai/keys")
        console.print("   â€¢ Add to .env: OPENROUTER_API_KEY=your_key")
        console.print()

    console.print("[green]âœ“ Testing complete![/green]\n")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\n[yellow]Tests interrupted by user[/yellow]\n")
        sys.exit(0)
    except Exception as e:
        console.print(f"\n[red]Test suite error: {e}[/red]\n")
        sys.exit(1)
